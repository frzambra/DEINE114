---
title: "Random Forest con {tidymodels}"
subtitle: "An√°lisis espacial con R"
author: "Dr. Francisco Zambrano"
format: 
  revealjs:
    slide-number: true
    logo: ../imgs/logo_hemera_2024.png
    css: ../logo.css
revealjs-plugins:
  - drop
drop:
  shortcut: "]"
editor: source
---

# Random Forest {background-color='darkgreen'}

## ¬øQu√© es machine learning? {.smaller background-image="../imgs/R_logo.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

![](figs/machine_learning.png){fig-align='center'}

## ¬øQu√© es machine learning? {.smaller background-image="../imgs/R_logo.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

![](figs/what_is_ml.jpg){fig-align='center'}

## ¬øQu√© es machine learning? {.smaller background-image="../imgs/R_logo.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

![](figs/ml_illustration.jpg)

## ¬øQu√© es Random Forest? {.smaller background-image="../imgs/R_logo.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

+ `Random Forest` es un m√©todo de `Machine Learning`  que se utiliza principalmente para tareas de clasificaci√≥n y regresi√≥n. 

+ Opera construyendo m√∫ltiples `√°rboles de decisi√≥n` durante el entrenamiento y fusionando sus resultados para mejorar la precisi√≥n y la solidez. 

+ Caracter√≠sticas clave:

  + `Aprendizaje conjunto`: Random Forest combina las predicciones de numerosos `√°rboles de decisi√≥n` para reducir el sobreajuste y mejorar la generalizaci√≥n.
  + `Bootstrap Aggregating (Bagging)`: utiliza una t√©cnica llamada `bagging`, donde se muestrean subconjuntos aleatorios de los datos de entrenamiento con reemplazo para crear diversos √°rboles.

## ¬øQu√© es Random Forest? {.smaller background-image="../imgs/R_logo.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

+ Caracter√≠sticas clave:

  + `Aleatoriedad de predictores (features)`: al dividir nodos durante la construcci√≥n del √°rbol, Random Forest selecciona aleatoriamente un subconjunto de predictores, lo que contribuye a√∫n m√°s a la diversidad de los √°rboles.
  + `Votaci√≥n/Promedio:` para las tareas de clasificaci√≥n, la predicci√≥n final se realiza mediante votaci√≥n mayoritaria entre los √°rboles, mientras que para las tareas de regresi√≥n, se promedian las predicciones.
  + `Robustez:` es menos sensible al ruido y puede manejar grandes conjuntos de datos con alta dimensionalidad de manera efectiva.

## ¬øQu√© es Random Forest? {.smaller background-image="../imgs/R_logo.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

Supuestos del Random Forest

+ `Independencia de los √°rboles:` los √°rboles de decisi√≥n del bosque deben ser independientes entre s√≠. Esto se logra mediante muestreo bootstrap y aleatoriedad de caracter√≠sticas.
+ `Datos suficientes:` Random Forest requiere una gran cantidad de datos para construir diversos √°rboles y lograr un rendimiento √≥ptimo.
+ `√Årboles equilibrados:` el algoritmo supone que los √°rboles individuales crecen lo suficientemente profundos como para capturar los patrones subyacentes en los datos.
+ `Manejo de datos ruidosos:` Random Forest puede manejar datos ruidosos, pero supone que el ruido se distribuye aleatoriamente y no es sistem√°tico.
    
## ¬øQu√© es Random Forest? {.smaller background-image="../imgs/R_logo.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

![](figs/decisiontree_and_randomforest.png){fig-align='center'}

## √Årboles de decisi√≥n {.smaller background-image="../imgs/R_logo.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

> Los modelos basados en √°rboles son una clase de algoritmos no param√©tricos que funcionan dividiendo el espacio de caracter√≠sticas en una serie de regiones m√°s peque√±as (no superpuestas) con valores de respuesta similares utilizando un conjunto de reglas de divisi√≥n.

![](figs/decision_trees_example.png){fig-align='center'}

## √Årboles de decisi√≥n {.smaller background-image="../imgs/R_logo.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

**Ejemplo simple**

![](figs/exemplar-decision-tree.png){fig-align='center'}

## √Årboles de decisi√≥n {.smaller background-image="../imgs/R_logo.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

**Estructura**

![](figs/decision-tree-terminology.png){fig-align='center'}

## √Årboles de decisi√≥n {.smaller background-image="../imgs/R_logo.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

Se puede utilizar para:

![](figs/regression-vs-classification.jpg){fig-align='center'}

## √Årboles de decisi√≥n {.smaller background-image="../imgs/R_logo.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

### ¬øC√≥mo se realiza la partici√≥n?

**Regresi√≥n**

CART utiliza particiones recursivas binarias (es recursiva porque cada divisi√≥n o regla depende de las divisiones que se encuentran sobre ella). 

El objetivo en cada nodo es encontrar la "mejor" caracter√≠stica $(x_i)$ para dividir los datos restantes en una de dos regiones (R1 y R2) de modo que el error general entre la respuesta real $(y_i)$ y la constante predicha $(c_i)$ se minimiza.

$$MSE= \sum_{i \epsilon R_1}^n(Y_i-c_1)^2 +\sum_{i \epsilon R_2}^n(Y_i-c_2)^2$$

Para problemas de clasificaci√≥n, la partici√≥n generalmente se realiza para maximizar la reducci√≥n de la entrop√≠a cruzada o el √≠ndice de Gini.

## √Årboles de decisi√≥n {.smaller background-image="../imgs/R_logo.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

::: {layout-ncol=2}
![](figs/decision-stump-1.png)

![](figs/decision-stump-2.png)
:::

## √Årboles de decisi√≥n {.smaller background-image="../imgs/R_logo.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

::: {layout-ncol=2}
![](figs/depth-3-decision-tree-1.png)

![](figs/depth-3-decision-tree-2.png)
:::

## √Årboles de decisi√≥n {.smaller background-image="../imgs/R_logo.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

**¬øCu√°ntas ramas debe tener el √°rbol?**

Si creamos un √°rbol demasiado complejo como en la figura, tendemos a sobreajustar nuestros datos de entrenamiento, lo que resulta en un rendimiento de generalizaci√≥n deficiente.

::: {layout-ncol=2}
![](figs/deep-overfit-tree-1.png){fig-align='center'}

![](figs/deep-overfit-tree-2.png){fig-align='center'}
:::

## √Årboles de decisi√≥n {.smaller background-image="../imgs/R_logo.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

**¬øCu√°ntas ramas debe tener el √°rbol?**

![](figs/dt-early-stopping-1.png){fig-align='center' width=70%}

## √Årboles de decisi√≥n {.smaller background-image="../imgs/R_logo.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

**Un ejemplo simple con meuse**

```{r}
#| echo: true
library(sp)
library(rpart)
library(rpart.plot)
data(meuse)
tree <- rpart(zinc~dist,meuse)
rpart.plot(tree)
```

## Random Forest {.smaller background-image="../imgs/R_logo.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

**¬øC√≥mo Funciona?**

`Bagging:` Bootstrap Aggregation, sirve como t√©cnica de ensamblado/conjunto.

![](figs/1_DvgOxmBc30t9HjDKFYLC0g.jpg){fig-align='center' width=80%}

## Random Forest {.smaller background-image="../imgs/R_logo.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

Pasos involucrados en Random Forest

+ `Paso 1:` en este modelo, se selecciona un subconjunto de puntos de datos y un subconjunto de caracter√≠sticas para construir cada √°rbol de decisi√≥n. En pocas palabras, se toman $n$ registros aleatorios y $m$ caracter√≠sticas del conjunto de datos que tiene $k$ n√∫mero de registros.
+ `Paso 2:` Se construyen √°rboles de decisi√≥n individuales para cada muestra.
+ `Paso 3:` Cada √°rbol de decisi√≥n generar√° un resultado.
+ `Paso 4:` El resultado final se considera en funci√≥n de la votaci√≥n mayoritaria (clasificaci√≥n) o el promedio (regresi√≥n).

## Random Forest {.smaller background-image="../imgs/R_logo.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

Random Forest utiliza estimaci√≥n del error out-of-bag (OOB) en cada submuestra.

+ El **OOB** es un m√©todo utilizado para estimar el error de predicci√≥n de Random Forest. 

+ Aprovecha el concepto de agregaci√≥n bootstrap, donde se crean m√∫ltiples subconjuntos del conjunto de datos original mediante muestreo aleatorio con reemplazo. 

+ Algunas instancias del conjunto de datos no se incluir√°n en estos subconjuntos para entrenar un √°rbol determinado. 

+ Estas instancias excluidas se conocen como muestras `"fuera de bolsa"` (OOB).

+ Para cada submuestra deja ~ $1/3$ para calcular el error de predicci√≥n (validaci√≥n cruzada)


## Random Forest {.smaller background-image="../imgs/R_logo.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

**¬øPor qu√© Random Forest es poderoso?**

+ `Precisi√≥n:` Random Forest a menudo proporciona una mayor precisi√≥n en comparaci√≥n con los √°rboles de decisi√≥n individuales porque la agregaci√≥n de predicciones reduce la varianza.
+ `Robustez:` Es menos propenso al sobreajuste debido a la aleatoriedad introducida en los procesos de arranque y selecci√≥n de caracter√≠sticas.
+ `Versatilidad:` puede manejar tareas de clasificaci√≥n y regresi√≥n y funciona bien con datos categ√≥ricos y num√©ricos.
+ `Importancia de las caracter√≠sticas:` Random Forest proporciona informaci√≥n sobre la importancia de las diferentes caracter√≠sticas en la predicci√≥n de la variable objetivo, lo que puede resultar √∫til para la selecci√≥n de caracter√≠sticas.

# Random Forest con {tidymodels} {background-color='orange'}

## ¬øQu√© es {tidymodels}? {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

```{r}
#| echo: true
library(tidymodels)
#> ‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels 1.0.0 ‚îÄ‚îÄ
#> ‚úî broom        1.0.0     ‚úî rsample      1.0.0
#> ‚úî dials        1.0.0     ‚úî tibble       3.1.8
#> ‚úî dplyr        1.0.9     ‚úî tidyr        1.2.0
#> ‚úî infer        1.0.2     ‚úî tune         1.0.0
#> ‚úî modeldata    1.0.0     ‚úî workflows    1.0.0
#> ‚úî parsnip      1.0.0     ‚úî workflowsets 1.0.0
#> ‚úî purrr        0.3.4     ‚úî yardstick    1.0.0
#> ‚úî recipes      1.0.1
#> ‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels_conflicts() ‚îÄ‚îÄ
#> ‚úñ purrr::discard() masks scales::discard()
#> ‚úñ dplyr::filter()  masks stats::filter()
#> ‚úñ dplyr::lag()     masks stats::lag()
#> ‚úñ recipes::step()  masks stats::step()
#> ‚Ä¢ Use tidymodels_prefer() to resolve common conflicts.
```

## ¬øQu√© es {tidymodels}? {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

![](figs/tidymodels_que_es.png){width=100% fig-align='center'}

## El juego completo {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

![](figs/tidy_models_whole_game.png){fig-align='center'}

## Divisi√≥n en entrenamiento y testeo {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

Para el aprendizaje autom√°tico, normalmente dividimos los datos en conjuntos de entrenamiento y testeo:

. . .

- El **set de entrenamiento** se utiliza para estimar los par√°metros del modelo.
- El **set de testeo** se utiliza para encontrar una evaluaci√≥n independiente del rendimiento del modelo.

. . .

No üö´ utilice el set de testeo durante el entrenamiento.

## Divisi√≥n en entrenamiento y testeo {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

![](figs/split_test_train.png){fig-align='center'}

## Divisi√≥n en entrenamiento y testeo {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

- Utilizar demasiados datos en **entrenamiento** nos impide calcular una buena evaluaci√≥n del **rendimiento** predictivo.

. . .

- Utilizar demasiados datos en **testeo** nos impide calcular una buena estimaci√≥n de los **par√°metros** del modelo.

## Divisi√≥n en entrenamiento y testeo {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

**Con los datos de meuse**

Hagamos la divisi√≥n de los datos 

```{r}
#| echo: true
set.seed(123)
meuse_split <- initial_split(meuse)
meuse_split
```

## Divisi√≥n en entrenamiento y testeo {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

**Con los datos de meuse**

```{r}
#| echo: true
meuse_train <- training(meuse_split)
meuse_test <- testing(meuse_split)
```

## Especificar un modelo {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

**Especificar un modelo**

+ Elegir el modelo

+ Elegir el motor

+ Elegir el modo

Todos los modelos disponibles se enumeran en [https://www.tidymodels.org/find/parsnip/](https://www.tidymodels.org/find/parsnip/)

## Especificar un modelo {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

```{r}
rf_spec <- rand_forest() |> 
  set_mode('regression') |> 
  set_engine('ranger')
```

## Modelo de flujo de trabajo (workflow) {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

Los flujos de trabajo manejan datos nuevos mejor que las herramientas b√°sicas de R en t√©rminos de nuevos niveles de factores

. . .

- Puedes usar otros preprocesadores adem√°s de f√≥rmulas (avanzados)

. . .

- Pueden ayudar a organizar su trabajo cuando trabaje con varios modelos.

. . .

- [Lo m√°s importante]{.underline}, un flujo de trabajo captura todo el proceso de modelado: `fit()` y `predict()` se aplican a los pasos de preprocesamiento adem√°s del ajuste real del modelo.

## Modelo de flujo de trabajo (workflow) {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

Sin workflows

```{r}
#| echo: true

rf_spec <- rand_forest() |> 
  set_mode('regression') |> 
  set_engine('ranger')

rf_spec |> 
  fit(zinc~dist+soil,data = meuse)
```

## Modelo de flujo de trabajo (workflow) {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

Usando workflows

```{r}
#| echo: true
workflow() |> 
  add_formula(zinc ~ dist+soil ) |> 
  add_model(rf_spec) |> 
  fit(data = meuse_train) 
```

## Predecir utilizando el modelo entrenado  {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

```{r}
#| echo: true

rf_fit <- workflow() |> 
  add_formula(zinc ~ dist+soil ) |> 
  add_model(rf_spec) |> 
  fit(data = meuse_train) 

predict(rf_fit, new_data = meuse_test)
```

## Evaluar las predicciones {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

```{r}
#| echo: true
augment(rf_fit, new_data = meuse_test)
```

## Evaluar las predicciones  {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

```{r}
#| echo: true
data_ev <- augment(rf_fit, new_data = meuse_test)
ggplot(data_ev,aes(.pred,zinc)) + 
  geom_point() + 
  geom_abline(slope =1,lty='dashed') + 
  theme_bw()
```

## Evaluar las predicciones  {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

::: {.columns}
::: {.column width=50%}

En el set de entrenamiento

```{r}
#| echo: true
augment(rf_fit, new_data = meuse_train) |> 
  metrics(truth = .pred, 
        estimate = zinc)
```
:::
::: {.column width=50%}

En el set de testeo

```{r}
#| echo: true
augment(rf_fit, new_data = meuse_test) |> 
  metrics(truth = .pred, 
        estimate = zinc)
```
:::
:::


## Evaluar las predicciones  {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

**Remuestreo**

![](figs/resampling.svg){fig-align='center'}

## Evaluar las predicciones  {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

**Remuestreo**

```{r}
#| echo: true
set.seed(123)
meuse_folds <- vfold_cv(meuse_train, v = 10)
meuse_folds
```

## Evaluar las predicciones  {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

**Remuestreo**

```{r}
#| echo: true
rf_wf <- workflow() |> 
  add_formula(zinc ~ dist+soil ) |> 
  add_model(rf_spec) 
```


```{r}
#| echo: true
meuse_res <- fit_resamples(rf_wf, meuse_folds)
meuse_res
```

## Evaluar las predicciones  {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

**Remuestreo**

Podemos medir la calidad de las predicciones utilizando s√≥lo remuestreo!!

::: {.columns}
::: {.column width=50%}
```{r}
#| echo: true
meuse_res |> 
  collect_metrics()
```
:::
::: {.column width=50%}
Si comparamos con las m√©tricas en el set de entrenamiento y de  testeo.

En set de entrenamiento RMSE = 214 y $R^2$=0.684.  

En set de testeo RMSE = 219 y $R^2$=0.66.
:::
:::


## El ajuste final  {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

* Supongamos que estamos contentos con nuestro modelo de bosque aleatorio.

* Ajustemos el modelo en el conjunto de entrenamiento y verifiquemos nuestro desempe√±o usando el conjunto de prueba.

* He mostrado las funciones `fit()` y `predict()` (`+ augment()`) pero hay un atajo:

```{r}
#| echo: true
final_fit <- last_fit(rf_wf, meuse_split) 
final_fit
```

## El ajuste final  {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

```{r}
#| echo: true
collect_metrics(final_fit)
```

. . .

Estas son las m√©tricas calculadas sobre el `set de testeo`.

## El ajuste final  {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

```{r}
#| echo: true
collect_predictions(final_fit)
```

## El ajuste final  {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

```{r}
#| echo: true
extract_workflow(final_fit)
```

. . .

Utilizar este modelo para realizar las predicciones.

## Todo el juego  {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

![](figs/tidy_models_whole_game.png){fig-align='center'}

## Ajustar los par√°metros del modelo  {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

Algunos par√°metros del modelo o de preprocesamiento no se pueden estimar directamente a partir de los datos.

Algunos ejemplos:

* Profundidad del √°rbol en √°rboles de decisi√≥n

* N√∫mero de vecinos en un modelo de vecino K-m√°s cercano

## Optimizar los par√°metros del modelo  {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

* Pruebe diferentes valores y mida su desempe√±o.

* Encuentre buenos valores para estos par√°metros.

* Una vez que se determinan los valores de los par√°metros, se puede finalizar un modelo ajust√°ndolo a todo el conjunto de entrenamiento.

## Optimizar los par√°metros del modelo  {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

Las dos estrategias principales de optimizaci√≥n son:

. . .

- **B√∫squeda de cuadr√≠cula (grid search)** üí† que prueba un conjunto predefinido de valores candidatos

- **B√∫squeda iterativa (iterative search)** üåÄ que sugiere/estima nuevos valores de par√°metros candidatos para evaluar

## Optimizar los par√°metros del modelo  {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

Probemos con el modelo de Random Forest para `meuse`.

```{r}
#| echo: true
rf_spec <- rand_forest(
  tree = 1000,
  mtry = tune(),
  min_n = tune()
  ) |> 
  set_mode('regression') |> 
  set_engine('ranger',importance ='impurity')

rf_wf <- workflow() |> 
  add_formula(zinc ~ dist+soil) |> 
  add_model(rf_spec) 
```

## Optimizar los par√°metros del modelo  {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

Probemos con el modelo de Random Forest para `meuse`.

```{r}
set.seed(22)
rf_res <- tune_grid(
  rf_wf,
  meuse_folds,
  grid = 5
)
```

## Optimizar los par√°metros del modelo  {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

```{r}
autoplot(rf_res)
```

## Optimizar los par√°metros del modelo  {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

Inspeccionando los resultados y seleccionad el de mejor calidad

```{r}
#echo: true
show_best(rf_res)
```

```{r}
#| echo: true
mejores_param <- select_best(rf_res)
```

## Ajuste final  {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

```{r}
#| echo: true
rf_wf <- finalize_workflow(rf_wf, mejores_param)
final_fit <- last_fit(rf_wf, meuse_split) 

collect_metrics(final_fit)
```


## Importancia de las variables  {.smaller background-image="https://juliasilge.github.io/tidymodels-tutorial/slides/hexes/tidymodels.png" background-position="97.5% 2.5%" background-size="7.5%" layout="true"}

```{r}
library(vip)
final_fit |>  
  extract_fit_parsnip() %>% 
  vip(num_features = 20)
```

# Random Forest para interpolaci√≥n espacial {background-color='gray'}

## Motivaci√≥n

![](figs/spRF_Hengl_et_al.png){fig-align = 'center'}
